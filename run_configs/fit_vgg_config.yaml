# lightning.pytorch==2.4.0
seed_everything: true
trainer:
  accelerator: "gpu"
  precision: null
  logger:
    class_path: 'lightning.pytorch.loggers.tensorboard.TensorBoardLogger'
    init_args:
      save_dir: './output_vgg'

  callbacks:
    - class_path: ModelCheckpoint
      init_args:
        filename: "dn_epoch{epoch:02d}_loss{epoch_val_loss:.2f}"
        auto_insert_metric_name: false
        save_last: true
        monitor: 'epoch_val_loss'
        mode: 'min'
        verbose: true
    - class_path: EarlyStopping
      init_args:
        monitor: 'epoch_val_loss'
        mode: 'min'
        patience: 4
        check_on_train_epoch_end: false
        verbose: true
    - class_path: LearningRateMonitor
      init_args:
        logging_interval: 'epoch'

  fast_dev_run: false
  max_epochs: 100
#  overfit_batches: 1
  check_val_every_n_epoch: 1
#  limit_val_batches: 0  # set to 0 to disable validation, but overwritten when overfit_batches is used
  num_sanity_val_steps: 0
  log_every_n_steps: 1
  inference_mode: true
  use_distributed_sampler: true

model:
  class_path: 'host_module.HostModule'
  init_args:
    arch: 'VGG'
    optimizer:
      class_path: 'torch.optim.AdamW'
      init_args:
        lr: 0.01
    scheduler:
      class_path: 'torch.optim.lr_scheduler.ReduceLROnPlateau'
      init_args:
        mode: 'max'
        factor: 0.5
        patience: 5
    scheduler_config: {monitor: 'epoch_val_loss', frequency: 1}
  dict_kwargs:
    blocks: [[1, 1, 16], [1, 16,  32], [2, 32, 64], [2, 64, 128], [2, 128, 128]]
#    blocks: [[1, 1, 64], [1, 64, 128],[2, 128, 256],[2, 256, 512],[2, 512, 512]]

#data:
#  class_path: 'data_module.ImagenetteDataModule'
#  init_args:
#    data_dir: './data'
#    batch_size: 1


data:
  class_path: 'data_module.FashionMNISTDataModule'
  init_args:
    data_dir: './data'
    batch_size: 256